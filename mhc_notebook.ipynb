{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aaae3bd",
   "metadata": {},
   "source": [
    "## The Necessity of Residual Connections\n",
    "\n",
    "In early deep learning (pre-2015), training very deep networks was notoriously difficult due to the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). As gradients backpropagate through many layers, they multiply, and if the eigenvalues of the weight matrices are small, the gradients shrink exponentially effectively \"killing\" the learning in earlier layers.\n",
    "\n",
    "### The Solution: $x_{l+1} = x_l + F(x_l)$\n",
    "\n",
    "The [Residual Connection](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) allows the gradient to flow directly through the identity path ($x_l$) unimpeded. This acts as a \"gradient superhighway.\"\n",
    "\n",
    "This simple addition is used in almost every major modern architecture:\n",
    "*   [ResNet](https://huggingface.co/docs/transformers/v4.32.0/model_doc/resnet) (Vision)\n",
    "*   [Transformers](https://huggingface.co/docs/transformers/index) (LLMs: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), [Llama](https://arxiv.org/pdf/2302.13971), [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)), etc.)\n",
    "\n",
    "Let's implement a standard Residual Block using [JAX NNX](https://flax.readthedocs.io/en/v0.8.3/experimental/nnx/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: In Colab 'tpu' runtime, JAX is usually pre-installed, but updating is good.\n",
    "# We install the \"cpu\" version of jax locally if no TPU/GPU is detected, strictly for compatibility.\n",
    "!pip install -q -U jax jaxlib flax optax ray[default,train,data] tensorflow-datasets clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c03e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from flax import nnx # Use the new NNX API\n",
    "import optax\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.data\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "print(f\"Ray Version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nnx.Module):\n",
    "    def __init__(self, channels, rngs: nnx.Rngs):\n",
    "        # A simple Pre-Activation ResNet block\n",
    "        self.norm1 = nnx.BatchNorm(channels, rngs=rngs)\n",
    "        self.conv1 = nnx.Conv(channels, channels, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.norm2 = nnx.BatchNorm(channels, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(channels, channels, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x, train=True):\n",
    "        # Identity path\n",
    "        residual = x\n",
    "        \n",
    "        # F(x) path\n",
    "        y = self.norm1(x, use_running_average=not train)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.conv1(y)\n",
    "        \n",
    "        y = self.norm2(y, use_running_average=not train)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.conv2(y)\n",
    "        \n",
    "        # Add residual\n",
    "        return residual + y\n",
    "\n",
    "class ResNetModel(nnx.Module):\n",
    "    def __init__(self, num_classes=100, width=64, depth=3, rngs: nnx.Rngs=None):\n",
    "        self.conv_in = nnx.Conv(3, width, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        \n",
    "        # Stack blocks\n",
    "        self.blocks = [\n",
    "            ResBlock(width, rngs=rngs) for _ in range(depth)\n",
    "        ]\n",
    "        \n",
    "        self.norm_final = nnx.BatchNorm(width, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(width, num_classes, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.conv_in(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, train=train)\n",
    "        \n",
    "        x = self.norm_final(x, use_running_average=not train)\n",
    "        x = nnx.relu(x)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f60282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Instantiation\n",
    "rngs = nnx.Rngs(0)\n",
    "model = ResNetModel(width=64, depth=4, rngs=rngs)\n",
    "dummy_input = jnp.ones((1, 32, 32, 3))\n",
    "output = model(dummy_input, train=False)\n",
    "print(f\"Model Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b76ce",
   "metadata": {},
   "source": [
    "## JAX NNX and Ray\n",
    "\n",
    "### JAX NNX\n",
    "JAX has traditionally relied on a purely functional programming model (stateless functions, immutable data). **NNX** is a new experimental API from Flax that brings a more familiar, Pythonic, Object-Oriented experience to JAX, similar to PyTorch, while retaining JAX's transform capabilities (`jit`, `grad`, `vmap`).\n",
    "\n",
    "In NNX:\n",
    "*   **Modules** are Python objects that hold state (`nnx.Param`, `nnx.Variable`).\n",
    "*   **State management** is explicit but handled for you during transforms.\n",
    "\n",
    "### Ray Data & Train\n",
    "*   **Ray Data:** A scalable data processing library for ML. It handles loading, transforming, and streaming data to training workers efficiently.\n",
    "*   **Ray Train:** A library for distributed training. It abstracts away the complexity of setting up multi-device (or multi-node) training loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea9730",
   "metadata": {},
   "source": [
    "## CIFAR-100 Dataset\n",
    "\n",
    "We will use **CIFAR-100**, which is significantly harder than CIFAR-10 (100 classes vs 10), making implementation differences more visible.\n",
    "\n",
    "We will use Ray Data to create a pipeline that:\n",
    "1.  Loads data from TensorFlow Datasets (TFDS).\n",
    "2.  Preprocesses images (normalization).\n",
    "3.  Batches and streams to the TPU/GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar100_streaming(split='train', batch_size=128):\n",
    "    # Load from TFDS\n",
    "    ds = ray.data.from_tfds(\n",
    "        tfds.load('cifar100', split=split, as_supervised=True)\n",
    "    )\n",
    "    \n",
    "    # Preprocessing\n",
    "    def preprocess(batch):\n",
    "        # Normalize to [0, 1] and then standardize\n",
    "        images = batch['image']\n",
    "        images = images.astype('float32') / 255.0\n",
    "        # Mean/Std for CIFAR-100\n",
    "        mean = np.array([0.5071, 0.4867, 0.4408], dtype='float32')\n",
    "        std = np.array([0.2675, 0.2565, 0.2761], dtype='float32')\n",
    "        images = (images - mean) / std\n",
    "        \n",
    "        batch['image'] = images\n",
    "        # Labels are ready \n",
    "        return batch\n",
    "\n",
    "    ds = ds.map_batches(preprocess, batch_format=\"numpy\")\n",
    "    \n",
    "    # In a real distributed setting with Ray Train, we would shard the dataset here.\n",
    "    # For a single-device notebook demo, we can just use the iterator.\n",
    "    # We use batch_format=\"numpy\" for direct JAX compatibility\n",
    "    return ds.iter_batches(batch_size=batch_size, batch_format=\"numpy\", prefetch_batches=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f93477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data\n",
    "sample_iter = get_cifar100_streaming(split='train', batch_size=4)\n",
    "sample_batch = next(sample_iter)\n",
    "print(f\"Sample Batch Shape: {sample_batch['image'].shape}\") \n",
    "# Note: Ray returns numpy arrays.\n",
    "# We will convert to JAX arrays inside the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3d7df",
   "metadata": {},
   "source": [
    "## 4. Hyper-Connections (HC)\n",
    "\n",
    "The concept of **Hyper-Connections** generalizes the Residual Connection. Instead of a fixed Identity mapping ($I \\cdot x$), we allow the network to *learn* how to pass information from layer $l$ to $l+1$ using a learnable linear transformation $H^{res}$.\n",
    "\n",
    "$$ x_{l+1} = H^{res} x_l + H^{post} F(H^{pre} x_l) $$\n",
    "\n",
    "Here, $x$ effectively contains multiple \"streams\" of information. $H^{res}$ allows these streams to mix linearly in the skip connection.\n",
    "\n",
    "### The Problem: Instability\n",
    "While this increases expressivity (the network can choose to ignore the skip, or amplify specific features), it introduces a major flaw: **Gain Unboundedness**.\n",
    "If the eigenvalues of $H^{res}$ deviate from 1, the signal magnitude can grow or shrink exponentially with depth, just like in plain non-residual networks. This makes training deep HC networks very unstable.\n",
    "\n",
    "Let's implement an HC Block where we split our channels into $N$ streams and learn the mixing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea75fc5",
   "metadata": {},
   "source": [
    "# Manifold-Constrained Hyper-Connections (mHC) with JAX NNX and Ray\n",
    "\n",
    "This notebook explores the concepts introduced in the paper **\"mHC: Manifold-Constrained Hyper-Connections\"**. We will explore the evolution of skip connections in deep learning, from standard Residual connections to Hyper-Connections, and finally to the proposed Manifold-Constrained Hyper-Connections.\n",
    "\n",
    "We will preserve the structure of the exploration:\n",
    "1.  **Residual Connections:** The foundation of modern LLMs.\n",
    "2.  **Hyper-Connections (HC):** A more expressive but unstable generalization.\n",
    "3.  **Manifold-Constrained Hyper-Connections (mHC):** The proposed solution using the Birkhoff Polytope.\n",
    "\n",
    "We will implement these models using **JAX NNX** (the new object-oriented neural network library for JAX) and train them using **Ray Data** and **Ray Train**.\n",
    "\n",
    "## Prerequisites & Installation\n",
    "\n",
    "We need to install JAX (with TPU support if on Colab), Ray, and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Training Loop using NNX\n",
    "def train_model(model_cls, model_kwargs, epochs=5, learning_rate=1e-3):\n",
    "    # Initialize Model & Optimizer\n",
    "    rngs = nnx.Rngs(42)\n",
    "    model = model_cls(**model_kwargs, rngs=rngs)\n",
    "    optimizer = nnx.Optimizer(model, optax.adamw(learning_rate))\n",
    "    \n",
    "    # Define Loss & Step\n",
    "    @nnx.jit\n",
    "    def train_step(model, optimizer, batch):\n",
    "        images, labels = jnp.array(batch['image']), jnp.array(batch['label'])\n",
    "        \n",
    "        def loss_fn(model):\n",
    "            logits = model(images, train=True)\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
    "            acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
    "            return loss, acc\n",
    "\n",
    "        # Compute gradients with NNX state handling\n",
    "        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, acc), grads = grad_fn(model)\n",
    "        \n",
    "        optimizer.update(grads)\n",
    "        return loss, acc\n",
    "\n",
    "    # Training\n",
    "    print(f\"Starting training for {model_cls.__name__}...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        data_iter = get_cifar100_streaming(split='train')\n",
    "        avg_loss = 0\n",
    "        avg_acc = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for i, batch in enumerate(data_iter):\n",
    "            # batch is dictionary from Ray\n",
    "            loss, acc = train_step(model, optimizer, batch)\n",
    "            avg_loss += loss\n",
    "            avg_acc += acc\n",
    "            steps += 1\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {i}, Loss: {loss:.4f}, Acc: {acc:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Completed. Avg Loss: {avg_loss/steps:.4f}, Avg Acc: {avg_acc/steps:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train Baseline (Short run for demo)\n",
    "# In a real notebook, increase epochs.\n",
    "train_model(ResNetModel, {'width': 64, 'depth': 4}, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6263276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCBlock(nnx.Module):\n",
    "    def __init__(self, channels, num_streams, rngs: nnx.Rngs):\n",
    "        self.num_streams = num_streams\n",
    "        self.channels = channels\n",
    "        \n",
    "        # H_res: Learnable mixing of streams in the skip connection\n",
    "        # Implemented as a Dense layer acting on the reshaped stream dimension\n",
    "        # Or a 1x1 Conv that mixes all channels\n",
    "        self.h_res = nnx.Linear(channels, channels, use_bias=False, rngs=rngs)\n",
    "        \n",
    "        # H_pre and H_post can be part of the Conv block F(x) usually, \n",
    "        # but here we make them explicit for the formula.\n",
    "        self.h_pre = nnx.Linear(channels, channels, use_bias=False, rngs=rngs)\n",
    "        self.h_post = nnx.Linear(channels, channels, use_bias=False, rngs=rngs)\n",
    "        \n",
    "        # Standard F(x)\n",
    "        self.norm = nnx.BatchNorm(channels, rngs=rngs)\n",
    "        self.conv = nnx.Conv(channels, channels, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        # x shape: (Batch, H, W, Channels)\n",
    "        \n",
    "        # 1. Skip Path: learnable mixing\n",
    "        skip = self.h_res(x)\n",
    "        \n",
    "        # 2. Residual Path: H_post * F(H_pre * x)\n",
    "        y = self.h_pre(x)\n",
    "        y = self.norm(y, use_running_average=not train)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.conv(y)\n",
    "        y = self.h_post(y)\n",
    "        \n",
    "        return skip + y\n",
    "\n",
    "class HCModel(nnx.Module):\n",
    "    def __init__(self, num_classes=100, width=64, depth=3, num_streams=4, rngs: nnx.Rngs=None):\n",
    "        self.conv_in = nnx.Conv(3, width, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.blocks = [HCBlock(width, num_streams, rngs=rngs) for _ in range(depth)]\n",
    "        self.norm_final = nnx.BatchNorm(width, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(width, num_classes, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.conv_in(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, train=train)\n",
    "        x = self.norm_final(x, use_running_average=not train)\n",
    "        x = nnx.relu(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        return self.linear_out(x)\n",
    "\n",
    "# Training HC Model\n",
    "# Note: This might be unstable or diverge if trained for long or if depth is large.\n",
    "train_model(HCModel, {'width': 64, 'depth': 4, 'num_streams': 4}, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596272e8",
   "metadata": {},
   "source": [
    "## 5. Manifold-Constrained Hyper-Connections (mHC)\n",
    "\n",
    "The **mHC** paper introduces a solution to the instability of Hyper-Connections: **Manifold Constraints**.\n",
    "Instead of allowing $H^{res}$ to be any matrix, we constrain it to the **Birkhoff Polytope** $\\mathcal{B}_N$, the set of doubly stochastic matrices.\n",
    "\n",
    "A Doubly Stochastic Matrix $M$ satisfies:\n",
    "1.  $M_{ij} \\ge 0$ (Non-negative)\n",
    "2.  $\\sum_j M_{ij} = 1$ (Rows sum to 1)\n",
    "3.  $\\sum_i M_{ij} = 1$ (Columns sum to 1)\n",
    "\n",
    "This constraint ensures that the gain of the residual path is bounded, preventing the signal explosion, while still allowing the network to dynamically mix and route information between streams.\n",
    "\n",
    "### Construction using Sinkhorn-Knopp\n",
    "We can learn an unconstrained parameter matrix $W$, and project it onto $\\mathcal{B}_N$ using the **Sinkhorn-Knopp algorithm** (iterative row and column normalization).\n",
    "\n",
    "$$ H^{res} = \\text{Sinkhorn}(\\phi(W)) $$\n",
    "\n",
    "where $\\phi$ ensures positivity (e.g., sigmoid/exp).\n",
    "\n",
    "Let's implement the Sinkhorn projection and the mHC Block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn Iteration in JAX\n",
    "@jax.jit\n",
    "def sinkhorn_knopp(log_alpha, n_iters=5):\n",
    "    # Start with exp(log_alpha) to ensure positivity\n",
    "    # Use log-space for numerical stability if needed, \n",
    "    # but for simple projection matrix operations, standard space is fine usually.\n",
    "    # Here we do standard space.\n",
    "    matrix = jnp.exp(log_alpha)\n",
    "    \n",
    "    def body_fn(i, mat):\n",
    "        # Normalize Rows\n",
    "        mat = mat / (jnp.sum(mat, axis=-1, keepdims=True) + 1e-6)\n",
    "        # Normalize Cols\n",
    "        mat = mat / (jnp.sum(mat, axis=-2, keepdims=True) + 1e-6)\n",
    "        return mat\n",
    "\n",
    "    matrix = jax.lax.fori_loop(0, n_iters, body_fn, matrix)\n",
    "    return matrix\n",
    "\n",
    "class mHCBlock(nnx.Module):\n",
    "    def __init__(self, channels, num_streams, rngs: nnx.Rngs):\n",
    "        self.num_streams = num_streams\n",
    "        self.channels = channels\n",
    "        self.dim_per_stream = channels // num_streams\n",
    "        \n",
    "        # Raw parameters for the mixing matrix (num_streams x num_streams)\n",
    "        # We start near identity-like behavior or random\n",
    "        self.h_res_logits = nnx.Param(jax.random.normal(rngs.params(), (num_streams, num_streams)))\n",
    "        \n",
    "        self.conv = nnx.Conv(channels, channels, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.norm = nnx.BatchNorm(channels, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x, train=True):\n",
    "        # x shape: (B, H, W, C)\n",
    "        B, H, W, C = x.shape\n",
    "        \n",
    "        # 1. Compute Projected H_res\n",
    "        h_res = sinkhorn_knopp(self.h_res_logits) # (S, S)\n",
    "        \n",
    "        # 2. Reshape x to apply mixing on streams\n",
    "        # (B, H, W, S, D)\n",
    "        x_reshaped = x.reshape(B, H, W, self.num_streams, self.dim_per_stream)\n",
    "        \n",
    "        # 3. Apply H_res: \"S S, ... S D -> ... S D\" \n",
    "        # We want to mix the 'S' dimension.\n",
    "        # einsum: i j (mixing) * ... j k (streams) -> ... i k\n",
    "        skip = jnp.einsum('ij, bhwjk -> bhwik', h_res, x_reshaped)\n",
    "        skip = skip.reshape(B, H, W, C)\n",
    "        \n",
    "        # 4. Feedforward Path (Standard)\n",
    "        y = self.norm(x, use_running_average=not train)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.conv(y)\n",
    "        \n",
    "        return skip + y\n",
    "\n",
    "class mHCModel(nnx.Module):\n",
    "    def __init__(self, num_classes=100, width=64, depth=3, num_streams=8, rngs: nnx.Rngs=None):\n",
    "        self.conv_in = nnx.Conv(3, width, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.blocks = [mHCBlock(width, num_streams, rngs=rngs) for _ in range(depth)]\n",
    "        self.norm_final = nnx.BatchNorm(width, rngs=rngs)\n",
    "        self.linear_out = nnx.Linear(width, num_classes, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        x = self.conv_in(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, train=train)\n",
    "        x = self.norm_final(x, use_running_average=not train)\n",
    "        x = nnx.relu(x)\n",
    "        x = jnp.mean(x, axis=(1, 2))\n",
    "        return self.linear_out(x)\n",
    "\n",
    "# Train mHC Model\n",
    "train_model(mHCModel, {'width': 64, 'depth': 4, 'num_streams': 8}, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f86b4",
   "metadata": {},
   "source": [
    "## 6. Comparison and Conclusion\n",
    "\n",
    "We have implemented three architectures:\n",
    "1.  **Residual Network:** Uses Identity $I$ for skip connections.\n",
    "2.  **Hyper-Connected Network:** Uses unconstrained $H$ for skip connections.\n",
    "3.  **mHC Network:** Uses Doubly Stochastic $P_M$ for skip connections.\n",
    "\n",
    "The **mHC** architecture provides a middle ground: it offers more expressivity than strict Identity (allowing cross-stream routing) while maintaining the stability guarantees crucial for deep model training, preventing the unbounded gain problem associated with raw Hyper-Connections.\n",
    "\n",
    "### Next Steps / Exercises\n",
    "*   Increase the depth to 50+ layers to see the instability of HC vs mHC more clearly.\n",
    "*   Visualize the learned `h_res` matrices in mHC to see how information routes between streams (e.g., is it learning to permute? broadcast?).\n",
    "*   Scale up to a Transformer architecture (mHC-Attention) as described in the paper.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
